{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7d3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbde61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS_JSON_PATH = \"../data/tools.json\"\n",
    "TOOLS_DESCRIPTION_JSON_PATH = \"../data/tool_descriptions.json\"\n",
    "\n",
    "tools_description_map = {}\n",
    "with open(TOOLS_DESCRIPTION_JSON_PATH, \"r\") as f:\n",
    "    tools_description_map = json.load(f)\n",
    "\n",
    "with open(TOOLS_JSON_PATH, \"r\") as f:\n",
    "    tools = json.load(f)\n",
    "\n",
    "tools_map = []\n",
    "tools_to_integration_map = {}\n",
    "for category, tools in tools.items():\n",
    "    for tool in tools:\n",
    "        tools_map.append(tool[\"name\"])\n",
    "        tools_to_integration_map[tool[\"name\"]] = category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "247f0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_descriptions_to_tool_name = {}\n",
    "for tool_name, tool_description in tools_description_map.items():\n",
    "    tool_descriptions_to_tool_name[tool_description] = tool_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60278ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data_files = glob.glob(\"../data/sft_data/*.json\")\n",
    "data_df = None\n",
    "for i, file in enumerate(all_data_files):\n",
    "    if data_df is None:\n",
    "        data_df = pd.read_json(file)\n",
    "    else:\n",
    "        data_df = pd.concat([data_df, pd.read_json(file)])\n",
    "\n",
    "assert data_df is not None\n",
    "\n",
    "tool_set = set()\n",
    "for i, tools_required in tqdm(enumerate(data_df['tools_required'].to_list())):\n",
    "    tool_list = []\n",
    "    for tool in tools_required:\n",
    "        tool_name = tool.split('-')[0].split(' ')[0]\n",
    "        # data_df.loc[i, tool_name] = True\n",
    "        tool_set.add(tool_name)\n",
    "        # tool_list.append(tool_name)\n",
    "    # data_df.loc[i, 'tools_required_list'] = tool_list\n",
    "\n",
    "tool_set = list(tool_set)\n",
    "print(len(tool_set))\n",
    "for tool_name in tool_set:\n",
    "    data_df[tool_name] = data_df['tools_required'].apply(lambda x: tool_name in x)\n",
    "data_df.to_pickle(\"../data/data_df.pkl\")\n",
    "# data_df.to_pickle(\"../data/sft_data/data_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bedf65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle(\"../data/data_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a947ea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'create_event': 'Calendar',\n",
       " 'update_event': 'Calendar',\n",
       " 'delete_event': 'Calendar',\n",
       " 'search_events': 'Calendar',\n",
       " 'create_file': 'Files',\n",
       " 'update_file': 'Files',\n",
       " 'trash_file': 'Files',\n",
       " 'start_call': 'FaceTime',\n",
       " 'send_email': 'Mail',\n",
       " 'search_inbox': 'Mail',\n",
       " 'get_directions': 'Maps',\n",
       " 'search_location': 'Maps',\n",
       " 'play_song': 'Music',\n",
       " 'search_library': 'Music',\n",
       " 'send_message': 'Slack',\n",
       " 'search_messages': 'Messages',\n",
       " 'create_note': 'Notes',\n",
       " 'update_note': 'Notes',\n",
       " 'create_reminder': 'Reminders',\n",
       " 'update_reminder': 'Reminders',\n",
       " 'capture_screenshot': 'System',\n",
       " 'find_symbol': 'Stocks',\n",
       " 'open_url': 'WebBrowser',\n",
       " 'get_weather': 'Weather',\n",
       " 'reply': 'DirectMessage',\n",
       " 'unachievable_task': 'Unachievable'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_to_integration_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9c9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466060, 31)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f91d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool not found: search_reminders\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m evals_json \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m data_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      3\u001b[0m     row_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtools_required\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     }\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtools_required\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/Router/.venv/lib/python3.10/site-packages/pandas/core/frame.py:1559\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1557\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1559\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1561\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Router/.venv/lib/python3.10/site-packages/pandas/core/series.py:594\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    592\u001b[0m NDFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m data_dtype:\n",
      "File \u001b[0;32m~/Router/.venv/lib/python3.10/site-packages/pandas/core/generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 814\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/Router/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m~/Router/.venv/lib/python3.10/site-packages/pandas/core/internals/base.py:92\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     89\u001b[0m old_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis])\n\u001b[1;32m     90\u001b[0m new_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_labels)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# If we are setting the index on a DataFrame with no columns,\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m#  it is OK to change the length.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evals_json = []\n",
    "\n",
    "for i, row in data_df.iterrows():\n",
    "    row_dict = {\n",
    "        \"query\": row['query'],\n",
    "        \"tools\": row['tools_required'],\n",
    "    }\n",
    "    if 'reply' in row['tools_required']:\n",
    "        row_dict['decision'] = 'integrations'\n",
    "        # print(f'Query: {row[\"query\"]}')\n",
    "        # print(f'Reply: {row[\"tools_required\"]}')\n",
    "        row_dict['tools'].append('send_message')\n",
    "    elif 'unachievable_task' in row['tools_required']:\n",
    "        row_dict['decision'] = 'unachievable'\n",
    "        row_dict['tools'] = []\n",
    "        # print(f'Query: {row[\"query\"]}')\n",
    "        # print(f'Unachievable: {row[\"tools_required\"]}')\n",
    "    else:\n",
    "        row_dict['decision'] = 'integrations'\n",
    "    integrations = []\n",
    "    for tool in row['tools_required']:\n",
    "        tool = tool.split('-')[0].split(' ')[0]\n",
    "        if tool in tools_to_integration_map:\n",
    "            integrations.append(tools_to_integration_map[tool])\n",
    "        else:\n",
    "            if 'Slack' in tool:\n",
    "                integrations.append('Slack')\n",
    "            elif 'message' in tool:\n",
    "                integrations.append('Messages')\n",
    "            else:\n",
    "                print(f'Tool not found: {tool}')\n",
    "\n",
    "    row_dict['tools'] = list(set(row_dict['tools']))\n",
    "    row_dict['integrations'] = list(set(integrations))\n",
    "    evals_json.append(row_dict)\n",
    "\n",
    "# with open(\"../data/evals.json\", \"w\") as f:\n",
    "#     json.dump(evals_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/evals_v0.json\", \"w\") as f:\n",
    "    json.dump(evals_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33122f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954422\n"
     ]
    }
   ],
   "source": [
    "# make pairs for each row\n",
    "positive_pairs = []\n",
    "for tool_name in tool_set:\n",
    "    filtered_df = data_df[data_df[tool_name] == True]\n",
    "    if tool_name not in tools_to_integration_map:\n",
    "        continue\n",
    "    integration_name = tools_to_integration_map[tool_name]\n",
    "    query_list = filtered_df['query'].tolist()\n",
    "    for query in query_list:\n",
    "        positive_pairs.append((query, integration_name))\n",
    "\n",
    "print(len(positive_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cee05f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Update the onboarding checklist file, create a note about changes, and send it to HR via Slack.',\n",
       " 'Slack')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "positive_pairs[random.randint(0, len(positive_pairs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21f2f1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 773081/773081 [00:03<00:00, 197388.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 85898/85898 [00:00<00:00, 140644.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 95443/95443 [00:00<00:00, 193616.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/positive_pairs.json\", field='data')\n",
    "dataset.shuffle()\n",
    "\n",
    "dataset_train_test = dataset['train'].train_test_split(test_size=0.1)\n",
    "dataset_test = dataset_train_test['test']\n",
    "dataset_train_val = dataset_train_test['train'].train_test_split(test_size=0.1)\n",
    "dataset_train = dataset_train_val['train']\n",
    "dataset_val = dataset_train_val['test']\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset_train,\n",
    "    \"val\": dataset_val,\n",
    "    \"test\": dataset_test\n",
    "})\n",
    "\n",
    "dataset_dict.save_to_disk(\"../data/positive_pairs_train_val_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "405d13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_val_test = load_from_disk(\"../data/positive_pairs_train_val_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0c44792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tool_descriptions =list(tools_description_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c477057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42700 43198\n",
      "48001 47442\n"
     ]
    }
   ],
   "source": [
    "queries = []\n",
    "tool_descriptions = []\n",
    "for i in range(len(dataset_train_val_test['val'])):\n",
    "    queries.append(dataset_train_val_test['val'][i]['text1'])\n",
    "    tool_descriptions.append(dataset_train_val_test['val'][i]['text2'])\n",
    "\n",
    "val_dataset = []\n",
    "no_of_positive_pairs = 0\n",
    "no_of_negative_pairs = 0\n",
    "\n",
    "for i, (q_i, tool_description_i) in enumerate(zip(queries, tool_descriptions)):\n",
    "    if random.random() < 0.5:\n",
    "        label = 1\n",
    "        tool_description = tool_description_i\n",
    "        no_of_positive_pairs += 1\n",
    "    else:\n",
    "        label = 0\n",
    "        tool_description = random.choice(list(set(all_tool_descriptions) - {tool_description_i}))\n",
    "        no_of_negative_pairs += 1\n",
    "\n",
    "    data = {\n",
    "        \"text1\": q_i,\n",
    "        \"text2\": tool_description,\n",
    "        \"label\": label\n",
    "    }\n",
    "    val_dataset.append(data)\n",
    "\n",
    "print(no_of_positive_pairs, no_of_negative_pairs)\n",
    "\n",
    "queries = []\n",
    "tool_descriptions = []\n",
    "for i in range(len(dataset_train_val_test['test'])):\n",
    "    queries.append(dataset_train_val_test['test'][i]['text1'])\n",
    "    tool_descriptions.append(dataset_train_val_test['test'][i]['text2'])\n",
    "    \n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "no_of_positive_pairs = 0\n",
    "no_of_negative_pairs = 0\n",
    "\n",
    "for i, (q_i, tool_description_i) in enumerate(zip(queries, tool_descriptions)):\n",
    "    if random.random() < 0.5:\n",
    "        label = 1\n",
    "        tool_description = tool_description_i\n",
    "        no_of_positive_pairs += 1\n",
    "    else:\n",
    "        label = 0\n",
    "        tool_description = random.choice(list(set(all_tool_descriptions) - {tool_description_i}))\n",
    "        no_of_negative_pairs += 1\n",
    "\n",
    "    data = {\n",
    "        \"text1\": q_i,\n",
    "        \"text2\": tool_description,\n",
    "        \"label\": label\n",
    "    }\n",
    "    test_dataset.append(data)\n",
    "\n",
    "print(no_of_positive_pairs, no_of_negative_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96008ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 773081/773081 [00:01<00:00, 735253.09 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 85898/85898 [00:00<00:00, 574763.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 95443/95443 [00:00<00:00, 622314.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset_train_val_test['train'],\n",
    "    \"val\": Dataset.from_list(val_dataset),\n",
    "    \"test\": Dataset.from_list(test_dataset)\n",
    "})\n",
    "\n",
    "dataset_dict.save_to_disk(\"../data/positive_pairs_train_val_test_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ab5139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text1', 'text2'],\n",
      "        num_rows: 773081\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text1', 'text2', 'label'],\n",
      "        num_rows: 85898\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text1', 'text2', 'label'],\n",
      "        num_rows: 95443\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_dict = load_from_disk(\"../data/positive_pairs_train_val_test_1\")\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd1ec22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text1', 'text2', 'label'],\n",
       "    num_rows: 95443\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36c2ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = [x['text1'] for x in dataset_dict['test']]\n",
    "all_queries_set = set(all_queries)\n",
    "\n",
    "data_df['test'] = data_df['query'].apply(lambda x: x in all_queries_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5b1b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_test = data_df[data_df['test'] == True]\n",
    "import random\n",
    "\n",
    "# Shuffle the test dataframe and select 500 examples\n",
    "data_df_test_shuffled = data_df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_df_test_500 = data_df_test_shuffled.head(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20126254",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_json = []\n",
    "\n",
    "for i, row in data_df_test_500.iterrows():\n",
    "    row_dict = {\n",
    "        \"query\": row['query'],\n",
    "        \"tools\": row['tools_required'],\n",
    "    }\n",
    "    if 'reply' in row['tools_required']:\n",
    "        row_dict['decision'] = 'integrations'\n",
    "        # print(f'Query: {row[\"query\"]}')\n",
    "        # print(f'Reply: {row[\"tools_required\"]}')\n",
    "        row_dict['tools'].append('send_message')\n",
    "    elif 'unachievable_task' in row['tools_required']:\n",
    "        row_dict['decision'] = 'unachievable'\n",
    "        row_dict['tools'] = []\n",
    "        # print(f'Query: {row[\"query\"]}')\n",
    "        # print(f'Unachievable: {row[\"tools_required\"]}')\n",
    "    else:\n",
    "        row_dict['decision'] = 'integrations'\n",
    "    integrations = []\n",
    "    for tool in row['tools_required']:\n",
    "        tool = tool.split('-')[0].split(' ')[0]\n",
    "        if tool in tools_to_integration_map:\n",
    "            integrations.append(tools_to_integration_map[tool])\n",
    "        else:\n",
    "            if 'Slack' in tool:\n",
    "                integrations.append('Slack')\n",
    "            elif 'message' in tool:\n",
    "                integrations.append('Messages')\n",
    "            else:\n",
    "                print(f'Tool not found: {tool}')\n",
    "\n",
    "    row_dict['tools'] = list(set(row_dict['tools']))\n",
    "    row_dict['integrations'] = list(set(integrations))\n",
    "    evals_json.append(row_dict)\n",
    "\n",
    "with open(\"../data/evals_test_500_v0.json\", \"w\") as f:\n",
    "    json.dump(evals_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "839c10af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c9780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
